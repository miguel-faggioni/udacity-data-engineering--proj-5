* TODO
** código
** README

* Data Pipelines - Udacity

  This repository is intended for the the fifth project of the Udacity Data Engineering Nanodegree Programa: Data Pipelines.

  The introduction was taken from the Udacity curriculum, since they summarize the activity better than I could.

** Introduction

A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

They have decided to bring you into the project and expect you to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

* TODO Folder structure

#+BEGIN_SRC 
/
├── airflow
│   ├── create_tables.sql - TODO
│   ├── dags - Airflow DAGs for the project
│   │   └── udac_example_dag.py - TODO
│   └── plugins - custom Airflow plugins for the project
│       ├── __init__.py - TODO
│       ├── helpers - TODO
│       │   ├── __init__.py - TODO
│       │   └── sql_queries.py - TODO
│       └── operators - custom Airflow operators for the project
│           ├── __init__.py - TODO
│           ├── data_quality.py - TODO
│           ├── load_dimension.py - TODO
│           ├── load_fact.py - TODO
│           └── stage_redshift.py - TODO
├── README.md - this file in markdown
└── README.org - this file in orgmode
#+END_SRC

* Airflow
** Installation

   In order to install Airflow locally, the following command must be run:

   #+BEGIN_SRC bash
   $ pip install apache-airflow==1.10.12 \
         --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.8.txt"
   #+END_SRC

   Then update the file `~/airflow/airflow.cfg` and set `dags_folder` to the dags folder of the project, and set `plugins_golder` to the plugins folder of the project. And run:

   #+BEGIN_SRC bash
   $ airflow initdb
   #+END_SRC

** Running

   After installing Airflow, run the following command:

   #+BEGIN_SRC bash
    $ airflow scheduler
   #+END_SRC

   And in another terminal:

   #+BEGIN_SRC bash
     $ airflow webserver
   #+END_SRC

   These will start the Airflow scheduler and webserver, respectively. The Airflow webserver will be available on `http://localhost:8080/`

* TODO Usage

  To run the pipeline, run the following snippet on the terminal:

  #+BEGIN_SRC bash
  python etl.py
  #+END_SRC

  This code depends on the AWS credentials stored in the ~dl.cfg~ file according to the following structure:

  #+BEGIN_SRC 
[AWS]
key=<AWS key>
secret=<AWS secret>

[S3]
input=<S3 bucket to load the data from>
output=<S3 bucket to store the processed data in>
  #+END_SRC

  The pipeline creates a spark session, processes the ~song_data~ folder on the input S3 bucket, then the ~log_data~ folder on the input S3 bucket. The schema needed to read the files is defined, so as to avoid any unforeseen data mixing up the expected column types.

  When saving the data into the output S3 bucket, each table is stored on their own folder in the S3 output bucket. And each folder is partitioned by the following columns:

  - the /songs/ table is partitioned by ~year~, then ~artist_id~;
  - the /artists/ table is partitioned by ~artist_id~;
  - the /users/ table is not partitioned;
  - the /time/ table is partitioned by ~year~, then ~month~;
  - and the /songplays/ table is partitioned by ~year~, then ~month~
